
@misc{Ostendorff2023clp,
  doi = {10.48550/ARXIV.2301.09626},
  author = {Ostendorff, Malte and Rehm, Georg},
  title = {Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning},
  publisher = {arXiv},
  year = {2023}
}

@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop and : and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Daniel McDuff and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023trustworthy,
      title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment}, 
      author={Yang Liu and Yuanshun Yao and Jean-Francois Ton and Xiaoying Zhang and Ruocheng Guo and Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hang Li},
      year={2023},
      eprint={2308.05374},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{liu2023geval,
      title={G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment}, 
      author={Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
      year={2023},
      eprint={2303.16634},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Chen_2019,
	doi = {10.1145/3292500.3330725},
  
	url = {https://doi.org/10.1145%2F3292500.3330725},
  
	year = 2019,
	month = {7},
  
	publisher = {ACM},
  
	author = {Qibin Chen and Junyang Lin and Yichang Zhang and Hongxia Yang and Jingren Zhou and Jie Tang},
  
	title = {Towards Knowledge-Based Personalized Product Description Generation in E-commerce},
  
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&}amp$\mathsemicolon$ Data Mining}
}


@inproceedings{wang-etal-2017-statistical,
	author = {Wang, Jinpeng and Hou, Yutai and Liu, Jing and Cao, Yunbo and Lin, Chin-Yew},
	title = {A Statistical Framework for Product Description Generation},
	booktitle = {International Joint Conference on Natural Language Processing},
	year = {2017},
	month = {11},
	abstract = {We present in this paper a statistical framework that generates accurate and fluent product description from product attributes. Specifically, after extracting templates and learning writing knowledge from attribute-description parallel data, we use the learned knowledge to decide what to say and how to say for product description generation. To evaluate accuracy and fluency for the generated descriptions, in addition to BLEU and Recall, we propose to measure what to say (in terms of attribute coverage) and to measure how to say (by attribute-specified generation) separately. Experimental results show that our framework is effective.},
	publisher = {Asian Federation of Natural Language Processing},
	url = {https://www.microsoft.com/en-us/research/publication/a-statistical-framework-for-product-description-generation/},
	pages = {187-192},
}
@inproceedings{langkilde-knight-1998-generation-exploits,
	author = {Langkilde, Irene and Knight, Kevin},
	title = {Generation That Exploits Corpus-Based Statistical Knowledge},
	year = {1998},
	publisher = {Association for Computational Linguistics},
	address = {USA},
	url = {https://doi.org/10.3115/980845.980963},
	doi = {10.3115/980845.980963},
	abstract = {We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.},
	booktitle = {Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1},
	pages = {704–710},
	numpages = {7},
	location = {Montreal, Quebec, Canada},
	series = {ACL '98/COLING '98}
}


@inproceedings{Fan_2022,
	doi = {10.1145/3534678.3539149},
  
	url = {https://doi.org/10.1145%2F3534678.3539149},
  
	year = 2022,
	month = {8},
  
	publisher = {ACM},
  
	author = {Xiaochuan Fan and Chi Zhang and Yong Yang and Yue Shang and Xueying Zhang and Zhen He and Yun Xiao and Bo Long and Lingfei Wu},
  
	title = {Automatic Generation of Product-Image Sequence in E-commerce},
  
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining}
}
@inproceedings{10.1145/3604915.3610647,
author = {Acharya, Arkadeep and Singh, Brijraj and Onoe, Naoyuki},
title = {LLM Based Generation of Item-Description for Recommendation System},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3610647},
doi = {10.1145/3604915.3610647},
abstract = {The description of an item plays a pivotal role in providing concise and informative summaries to captivate potential viewers and is essential for recommendation systems. Traditionally, such descriptions were obtained through manual web scraping techniques, which are time-consuming and susceptible to data inconsistencies. In recent years, Large Language Models (LLMs), such as GPT-3.5, and open source LLMs like Alpaca have emerged as powerful tools for natural language processing tasks. In this paper, we have explored how we can use LLMs to generate detailed descriptions of the items. To conduct the study, we have used the MovieLens 1M dataset comprising movie titles and the Goodreads Dataset consisting of names of books and subsequently, an open-sourced LLM, Alpaca, was prompted with few-shot prompting on this dataset to generate detailed movie descriptions considering multiple features like the names of the cast and directors for the ML dataset and the names of the author and publisher for the Goodreads dataset. The generated description was then compared with the scraped descriptions using a combination of Top Hits, MRR, and NDCG as evaluation metrics. The results demonstrated that LLM-based movie description generation exhibits significant promise, with results comparable to the ones obtained by web-scraped descriptions.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1204–1207},
numpages = {4},
keywords = {automated content generation., NLP, Large Language Models (LLMs), web scraping},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@misc{liu2023agentbench,
      title={AgentBench: Evaluating LLMs as Agents}, 
      author={Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
      year={2023},
      eprint={2308.03688},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{li2023static,
      title={Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation}, 
      author={Jiatong Li and Rui Li and Qi Liu},
      year={2023},
      eprint={2309.04369},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{
	Vos_2023, 
	title={The anatomy of a perfect product description}, 
	url={https://www.getresponse.com/blog/product-description}, journal={Marketing Software by GetResponse}, publisher={GetResponse}, 
	author={Vos, Lesley}, 
	year={2023},
	 month={2}
} 

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{tunstall2022natural,
  title={Natural Language Processing with Transformers: Building Language Applications with Hugging Face},
  author={Tunstall, L. and von Werra, L. and Wolf, T.},
  isbn={9781098103248},
  lccn={2023275986},
  url={https://books.google.de/books?id=pNBpzwEACAAJ},
  year={2022},
  publisher={O'Reilly Media}
}

@book{indurkhya2010handbook,
  title={Handbook of Natural Language Processing},
  author={Indurkhya, N. and Damerau, F.J.},
  isbn={9781420085938},
  lccn={2009049507},
  series={Chapman \& Hall/CRC machine learning \& pattern recognition series},
  url={https://books.google.de/books?id=nK-QYHZ0-_gC},
  year={2010},
  publisher={CRC Press}
}

@misc{zhao2023survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{twain1880awfulgerman,
  author    = {Mark Twain},
  title     = {Die schreckliche deutsche Sprache / The Awful German Language},
  year      = {2009},
  url       = {https://www.amazon.de/schreckliche-deutsche-Sprache-German-Language/dp/3868200398},
}

@misc{Kenny,
	title={FSI language difficulty},
	url={https://www.fsi-language-courses.org/blog/fsi-language-difficulty/},
	journal={FSI Language Courses - Free Online Language Courses},
	author={Kenny, Darren}
} 

@misc{gaoprompt,
	author = {Gao, Andrew},
	title = {Prompt Engineering for Large Language Models (July 8, 2023)},
	year = {2023}, 
	url = {http://dx.doi.org/10.2139/ssrn.4504303}
}

@misc{chen2023unleashing,
	title={Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review}, 
	author={Banghao Chen and Zhaofeng Zhang and Nicolas Langrené and Shengxin Zhu},
	year={2023},
	eprint={2310.14735},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{Ademi_2022, title={Längstes Deutsches Wort 2023}, url={https://neuroflash.com/de/blog/laengstes-deutsches-wort-2023/}, journal={neuroflash}, author={Ademi, Florina}, year={2022}, month={11}} 

@misc{RyteWiki, title={Flesch reading ease}, url={https://en.ryte.com/wiki/Flesch_Reading_Ease}, journal={Flesch Reading Ease - Ryte Digital Marketing Wiki}, author={Wiki, Ryte}} 

@inproceedings{Compound_Splitting_German,
	author = {Marco, Marion},
	year = {2017},
	month = {01},
	pages = {161-166},
	title = {Simple Compound Splitting for German},
	doi = {10.18653/v1/W17-1722}
}

@article{radford2019language,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019}
}

@inproceedings{minixhofer-etal-2022-wechsel,
	title = {WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models},
	author = {Minixhofer, Benjamin  and
	Paischer, Fabian  and
	Rekabsaz, Navid},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	month = {7},
	year = {2022},
	address = {Seattle, United States},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2022.naacl-main.293},
	pages = {3992--4006},
	abstract = {Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method {--} called WECHSEL {--} to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses subword-based tokenization and learns an embedding for each subword. The tokenizer of the source model (in English) is replaced with a tokenizer in the target language and token embeddings are initialized such that they are semantically similar to the English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer the English RoBERTa and GPT-2 models to four languages (French, German, Chinese and Swahili). We also study the benefits of our method on very low-resource languages. WECHSEL improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.},
}

@misc{agrawal2023knowledge,
	title={Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey}, 
	author={Garima Agrawal and Tharindu Kumarage and Zeyad Alghami and Huan Liu},
	year={2023},
	eprint={2311.07914},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{Bilan_2023, title={Hallucinations in LLMS: What you need to know before integration}, url={https://masterofcode.com/blog/hallucinations-in-llms-what-you-need-to-know-before-integration#:~:text=In%20summary%2C%20LLM%20hallucination%20arises,patterns%20rather%20than%20factual%20accuracy.}, journal={Master of Code Global}, author={Bilan, Maryna}, year={2023}, month={11}} 

@misc{nlp_ai_2023, title={Natural language processing (NLP) - A complete guide}, url={https://www.deeplearning.ai/resources/natural-language-processing/}, journal={(NLP) [A Complete Guide]}, author={.ai, Deeplearning}, year={2023}} 

@misc{alyafeai2020survey,
	title={A Survey on Transfer Learning in Natural Language Processing}, 
	author={Zaid Alyafeai and Maged Saeed AlShaibani and Irfan Ahmad},
	year={2020},
	eprint={2007.04239},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{lin2021survey,
	title={A Survey of Transformers}, 
	author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
	year={2021},
	eprint={2106.04554},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@misc{Team_2023, title={How to write effective Amazon descriptions and bullet points}, url={https://www.nembol.com/ecommerce-tips/how-to-write-product-description-amazon}, journal={Nembol}, author={Team, Nembol}, year={2023}, month={11}} 


@MISC{amazon_data,
	url ={https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020},
	year={2020},
	author={PromptCloud}
}

@article{wikidata,
	author = {Vrande\v{c}i\'{c}, Denny and Kr\"{o}tzsch, Markus},
	title = {Wikidata: A Free Collaborative Knowledgebase},
	year = {2014},
	issue_date = {October 2014},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {57},
	number = {10},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2629489},
	doi = {10.1145/2629489},
	abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
	journal = {Commun. ACM},
	month = {9},
	pages = {78–85},
	numpages = {8}
}

@InProceedings{dbpedia,
	author={Auer, Sören
	and Bizer, Christian
	and Kobilarov, Georgi
	and Lehmann, Jens
	and Cyganiak, Richard
	and Ives, Zachary},
	editor={Aberer, Karl
	and Choi, Key-Sun
	and Noy, Natasha
	and Allemang, Dean
	and Lee, Kyung-Il
	and Nixon, Lyndon
	and Golbeck, Jennifer
	and Mika, Peter
	and Maynard, Diana
	and Mizoguchi, Riichiro
	and Schreiber, Guus
	and Cudre-Mauroux, Philippe},
	title={DBpedia: A Nucleus for a Web of Open Data},
	booktitle={The Semantic Web},
	year={2007},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={722--735},
	abstract={DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
	isbn={978-3-540-76298-0}
}

@misc{jiang2023mistral,
	title={Mistral 7B}, 
	author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year={2023},
	eprint={2310.06825},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{jphme2023,
	author = {Jan Philipp Harries},
	title = {EM German Mistral v01},
	howpublished = {\url{https://huggingface.co/jphme/em_german_mistral_v01}},
	year = {2023},
	publisher = {Hugging Face},
}
@inproceedings{Bleu,
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	title = {BLEU: a method for automatic evaluation of machine translation},
	year = {2002},
	publisher = {Association for Computational Linguistics},
	address = {USA},
	url = {https://doi.org/10.3115/1073083.1073135},
	doi = {10.3115/1073083.1073135},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	pages = {311–318},
	numpages = {8},
	location = {Philadelphia, Pennsylvania},
	series = {ACL '02}
}

@misc{Ner, title={What is named Entity recognition (ner)?: Definition from TechTarget}, url={https://www.techtarget.com/whatis/definition/named-entity-recognition-NER}, journal={WhatIs}, publisher={TechTarget}, author={Barney, Nick}, year={2023}, month={3}} 